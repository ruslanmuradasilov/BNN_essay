    Идея бинарной нейронной сети была впервые предложена Matthieu Courbariaux, где веса и функции активации используют только бинарные числа как и в обучении, так и в алгоритме обратного распространения ошибки с использованием метода градиентого спуска (SGD) \cite{first}.
    
    Чуть позже Mohammad Rastegari в модели XNOR-Net \cite{second} добавляет скалярное значение, чтобы компенсировать потерю информации во время бинаризации, который был получен из статистики весов и функций активации до бинаризации. Это улучшило общие показатели, но подсчет этого скалярного значения оказался дорогостоящим.
    
    Victor Zhou попытался обобщить квантизацию и использовал преимущество битовых операций для фиксированной точки данных, варьируя ее размерность \cite{third}. Zhou представил DoReFa-Net, модель с переменной размерностью весов, функций активации и даже вычисления градиента во время обратного распространения ошибки со значительно улучшенным временем обучения.
    
    Другие модели также добились положительных результатов. Zheng Tang ускорил время обучения, изучив как влияет скорость обучения на показатели нейронной сети и на колеблемость бинарных значений \cite{fourth}. Идея BNN+, созданная Sajad Darabi,также улучшает скорость обучения, использую другую эффективную функцию обратного распространения вместо импульсной \cite{fifth}.
    
    Сравнение результатов всех этих моделей на разных датасетах были подробно описаны в работе Taylor Simons и Dah-Jye Lee \cite{sixth}. Наглядно видно, что на MNIST, SVHN, CIFAR и ImageNet датасетах бинарные нейронные сети практически не уступают своим вещественнным аналогам.